{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "390601e9",
   "metadata": {},
   "source": [
    "# Selection et entrainement des modèles "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8b13eb",
   "metadata": {},
   "source": [
    "***Résumé Exécutif du Feature Engineering - Prévision Solaire J+1***\n",
    "\n",
    "L'étape de feature_engineering a consisté à la prise en compte des conclusions de l'analyse exploratoire (EDA), ainsi que la création de garde-fous données manquantes/outliers ainsi que la création de features cycliques et de variables laggées.\n",
    "\n",
    "**Données** : Dataset horaire sur 2.5 ans (01/2023 - 05/2025) de production RTE et prévisions OpenMétéo. Identification de 2 valeurs manquantes nocturnes dans la cible, qui seront imputées à 0.\n",
    "\n",
    "**Gardes-fous :**\n",
    "- Création de tests IQR/Z-score et raise des outliers en intersection des deux filtres ;\n",
    "- Interpolation des séquences temporelles inférieures à 3h consécutives ;\n",
    "- Lors d'une absence de séquence de plus de 3 heures, création d'un reporting des séquences les plus longues, et potentiellement création d'un futur algorithme KNN - Filtre de Kalman.\n",
    "\n",
    "**Features créées :**\n",
    "- Création de features cycliques heures + mois, en fonction de la saisonnalité du cycle solaire ;\n",
    "- Création de features laggées (data leakage évité): \n",
    "  - Retard de 24, 32 et 48 (observation des cross-correlation + cohérent physiquement) ;\n",
    "  - Moyennes mobiles de 24, 32 et 48 périodes (analogue aux features retard).\n",
    " \n",
    "**Etapes effectuées dans ce notebook** :\n",
    "\n",
    "- Transformations statistiques pour les données LSTM et/ou SARIMAX si nécessaire ;\n",
    "- Baseline SARIMAX avec les features physiques les plus corrélées ;\n",
    "- Sélection de features (Embedding via LightGBM) ;\n",
    "- Validation croisée \"Expanding Window\" avec optimisation des hyperparamètres (Optuna) ;\n",
    "- Développement de modèles LightGBM et LSTM, avec MC dropout et regression quantile pour quantifier l'incertitude des modèles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d73410",
   "metadata": {},
   "source": [
    "### Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c5ca856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, Dict\n",
    "import optuna\n",
    "import traceback\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Modèles\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# Solaire\n",
    "from astral import LocationInfo\n",
    "from astral.sun import sun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2ef55cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirmation d'être à la racine du dossier\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "os.chdir(project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3becbebd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c62b2914",
   "metadata": {},
   "source": [
    "## Préparation des jeux de validation croisée"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d67cd7",
   "metadata": {},
   "source": [
    "La validation croisée (CV) est effectuée avec la validation croisée imbriquée (nested CV). Pour l'optimisation des hyperparamètres dans chaque fold, nous utiliserons l'alogrithme TPE supporté par Optuna. Du pruning sera effectué sur l'entrainement du LSTM Seq2seq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2c4f75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/processed/df_engineered.csv\", index_col=0)\n",
    "df_power = pd.read_csv(\"data/processed/occitanie_installed_power.csv\", index_col=0)\n",
    "df.index = pd.to_datetime(df.index, utc=True).tz_convert(\"Europe/Paris\")\n",
    "df_power.index = pd.to_datetime(df_power.index, utc=True).tz_convert(\"Europe/Paris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72777dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solar_power_norm(df_hourly_puiss: pd.DataFrame, \n",
    "                     df_installed_power: pd.DataFrame,\n",
    "                     col_hourly_puiss: str, \n",
    "                     col_installed_power: str) -> pd.Series:\n",
    "    \n",
    "    \"\"\"Prends en entrée la puissance produite horaire (df_hourly_puiss), et la normalise par la capacité régionale de production solaire (df_installed_power)\n",
    "\n",
    "    Args:\n",
    "        df_hourly_puiss (pd.DataFrame): DataFrame contenant les puissances solaires produites à la maille horaire\n",
    "        df_installed_power (pd.DataFrame): DataFrame contenant la capacité régionale solaire  à la maille horaire\n",
    "        col_daily_puiss (str): Nom de la colonne des puissances solaires\n",
    "        col_installed_power (str): Nom de la colonne de la capacité régionale solaire\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Colonne contenant les puissances solaires normalisées par la capacité régionale\n",
    "    \"\"\"\n",
    "    # if len(df_hourly_puiss) != len(df_installed_power):\n",
    "    #     raise ValueError(f\"Les deux dataframes n'ont pas la même longueur\"\n",
    "    #                      f\" {len(df_hourly_puiss)} vs {len(df_installed_power)}\")\n",
    "    \n",
    "    return df_hourly_puiss[col_hourly_puiss]/df_installed_power[col_installed_power]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ace9b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_nn(df: pd.DataFrame, target_col: str, features_to_use: Optional[list[str]] = None):\n",
    "    \"\"\"Retourne pour dataframe df les données d'entrée du modèle X transformée par MinMaxScaler (X_scaled) et la colonne cible (target_col)\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame df avec toutes les colonnes, colonne cible comprise.\n",
    "        target_col (str): Colonne cible.\n",
    "        features_to_use (Optional[list[str]], optional): Features à utiliser (optionnel). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        X_scaled (pd.DataFrame), y (pd.Series) : DataFrame scalé et colonne cible.\n",
    "    \"\"\"\n",
    "    if features_to_use:\n",
    "        X = df[features_to_use]\n",
    "    else:\n",
    "        X = df.drop(columns=target_col)\n",
    "    y = df[target_col]\n",
    "\n",
    "    # Scaling des features\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = pd.DataFrame(scaler.fit_transform(X), index = X.index, columns=X.columns)\n",
    "\n",
    "    return X_scaled, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eff322cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_fold(X: pd.DataFrame, y: pd.Series):\n",
    "    \"\"\"\n",
    "    Génère des splits temporels (TimeSeriesSplit) en conservant les index.\n",
    "    \n",
    "    Args:\n",
    "        X (pd.DataFrame): Variables explicatives.\n",
    "        y (pd.Series): Variable cible.\n",
    "    \n",
    "    Returns:\n",
    "        dict_ts_fold (dict): Dictionnaire contenant les folds de train/test.\n",
    "    \"\"\"\n",
    "    dict_ts_fold = {}\n",
    "    tscv = TimeSeriesSplit(gap=0, n_splits=5)\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(tscv.split(X)):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        dict_ts_fold[f'train_{i}'] = [X_train, y_train]\n",
    "        dict_ts_fold[f'test_{i}'] = [X_test, y_test]\n",
    "    \n",
    "    return dict_ts_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de14b0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outer_time_series_fold(X: pd.DataFrame, y: pd.Series, outer_n_splits: int = 5):\n",
    "    \"\"\"Effectue une validation croisée externe avec TimeSeriesSplit\"\"\"\n",
    "\n",
    "    dict_outer_fold = {}\n",
    "    tscv = TimeSeriesSplit(gap=0, n_splits=outer_n_splits)\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(tscv.split(X)):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index] \n",
    "        dict_outer_fold[f'train_{i}'] = [X_train, y_train]\n",
    "        dict_outer_fold[f'val_{i}'] = [X_test, y_test]\n",
    "    \n",
    "    return dict_outer_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3dcea695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_time_series_fold(folds: dict, inner_n_splits : int = 3):\n",
    "    \"\"\"Effectue une validation croisée interne (nested CV) sur les folds d'entraînement\"\"\"\n",
    "    \n",
    "    dict_inner_fold = {}\n",
    "    tscv = TimeSeriesSplit(gap=0, n_splits=inner_n_splits)\n",
    "\n",
    "    for key, (X_train_outer, y_train_outer) in folds.items():\n",
    "        \n",
    "        if key.startswith(\"train\"):\n",
    "            \n",
    "            for inner_i, (train_index, test_index) in enumerate(tscv.split(X_train_outer)):\n",
    "                X_train_inner, X_val_inner = X_train_outer.iloc[train_index], X_train_outer.iloc[test_index]\n",
    "                y_train_inner, y_val_inner = y_train_outer.iloc[train_index], y_train_outer.iloc[test_index]\n",
    "                dict_inner_fold[f'{key}_inner_train_{inner_i}'] = [X_train_inner, y_train_inner]\n",
    "                dict_inner_fold[f'{key}_inner_val_{inner_i}'] = [X_val_inner, y_val_inner]\n",
    "    \n",
    "    return dict_inner_fold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b17439b",
   "metadata": {},
   "source": [
    "### Baseline SARIMAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da914fd",
   "metadata": {},
   "source": [
    "Les features choisies pour la baseline SARIMAX sont celles ayant un impact physique sur la production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47aaab3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sun_data_for_dates(observer, start_date_ts, end_date_ts):\n",
    "    \"\"\"\n",
    "    Génère un DatetimeIndex pour toutes les heures entre l'aube et le crépuscule\n",
    "    pour chaque jour d'une période donnée.\n",
    "    \"\"\"\n",
    "    # Créer une plage de dates pour tous les jours de la période\n",
    "    dates = pd.date_range(start=start_date_ts, end=end_date_ts, freq=\"D\", tz=\"Europe/Paris\").date\n",
    "\n",
    "    # Initialiser l'index combiné\n",
    "    combined_index = pd.DatetimeIndex([], tz=\"Europe/Paris\")\n",
    "\n",
    "    for day in dates:\n",
    "        s = sun(observer, date=day)\n",
    "        dawn_local = pd.Timestamp(s['dawn']).tz_convert(\"Europe/Paris\")\n",
    "        dusk_local = pd.Timestamp(s['dusk']).tz_convert(\"Europe/Paris\")\n",
    "        temp_range = pd.date_range(start=dawn_local, end=dusk_local, freq=\"h\").floor(\"h\")\n",
    "        combined_index = combined_index.union(temp_range)\n",
    "\n",
    "    return combined_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "296d83fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information for Central/France\n",
      "Timezone: Europe/Paris\n",
      "Latitude: 43.62; Longitude: 2.34\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Feature physique\n",
    "city = LocationInfo(name=\"Central\", region=\"France\", timezone=\"Europe/Paris\", latitude=43.62476, longitude=2.34041)\n",
    "print((\n",
    "    f\"Information for {city.name}/{city.region}\\n\"\n",
    "    f\"Timezone: {city.timezone}\\n\"\n",
    "    f\"Latitude: {city.latitude:.02f}; Longitude: {city.longitude:.02f}\\n\"\n",
    "))\n",
    "start_ts = df_power.index.min().floor(\"d\")\n",
    "end_ts = df_power.index.max().floor(\"d\")\n",
    "\n",
    "# is_day\n",
    "index_time = get_sun_data_for_dates(city.observer, start_ts, end_ts)\n",
    "index_df = pd.DataFrame(index=index_time)\n",
    "index_df[\"is_day\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d529a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sarimax = ['temperature_2m', 'relative_humidity_2m', 'precipitation',\n",
    "       'surface_pressure', 'cloud_cover', 'wind_speed_10m',\n",
    "       'wind_direction_10m', 'global_tilted_irradiance',\n",
    "       'global_tilted_irradiance_delta_minmax', 'global_tilted_irradiance_std'] \n",
    "col_hourly_puiss = \"solar_mw\"\n",
    "col_installed_power = \"chronique_capacity\"\n",
    "solar_mw_normalized = solar_power_norm(df, df_power, col_hourly_puiss, col_installed_power)\n",
    "solar_mw_normalized = solar_mw_normalized.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d08b9f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sarimax(X_train: pd.DataFrame, \n",
    "                   y_train: pd.Series, \n",
    "                   X_val: pd.DataFrame, \n",
    "                   y_val: pd.Series, \n",
    "                   num_trials: Optional[int|None]) -> tuple:\n",
    "    \n",
    "    \"\"\"Entrainement d'un modèle SARIMAX avec nombre d'essais pour optimisation.\n",
    "    Retourne la quantification de son erreur (best_rmse) et ses hyperparamètres \n",
    "    optimaux (dict_best_params) sur un dataset de validation (X_val, y_val)\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Dataset d'entrainement\n",
    "        y_train (pd.Series): Variable cible d'entrainement\n",
    "        X_val (pd.DataFrame): Dataset de validation\n",
    "        y_val (pd.Series): Variable cible de validation\n",
    "\n",
    "    Returns:\n",
    "        best_rmse (float), dict_best_params (Dict) : Erreur (best_rmse) \n",
    "        et ses hyperparamètres optimaux (dict_best_params)\n",
    "    \"\"\"\n",
    "    # Initialisation\n",
    "    best_rmse = np.inf\n",
    "    dict_best_params = {}\n",
    "    \n",
    "    def objective_sarimax(trial) -> np.float64 :\n",
    "        \"\"\"Prend en entrée un set d'hyperparamètres SARIMAX issus du sampler d'Optuna, \n",
    "        et retourne le RMSE associé.\n",
    "\n",
    "        Args:\n",
    "            trial : Set d'hyperparamètres SARIMAX\n",
    "\n",
    "        Returns:\n",
    "            rmse (np.float64): Racine carrée de l'erreur quadratique moyenne du modèle\n",
    "        \"\"\"\n",
    "        # HP\n",
    "        d = 0\n",
    "        s = 24\n",
    "\n",
    "        order = (trial.suggest_int(\"p\", 0, 3), \n",
    "                d, \n",
    "                trial.suggest_int(\"q\", 0, 3))\n",
    "        \n",
    "        seasonal_order = (trial.suggest_int(\"P\", 0, 2), \n",
    "                          trial.suggest_int(\"D\", 0, 1), \n",
    "                          trial.suggest_int(\"D\", 0, 1), s)\n",
    "\n",
    "        # Model\n",
    "        model = SARIMAX(endog=y_train, \n",
    "                            exog=X_train, \n",
    "                            order = order, \n",
    "                            seasonal_order = seasonal_order)\n",
    "        \n",
    "        # Training\n",
    "        try:\n",
    "            fitted_model = model.fit(disp=False, maxiter=100)\n",
    "            predictions = fitted_model.get_prediction(X_val)\n",
    "            rmse = np.sqrt(mean_squared_error(y_val, predictions))\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.info(\"Echec de l'entrainement du modèle%s\", e)\n",
    "            logging.debug(\"Détails complets :\\n%s\", traceback.format_exc())\n",
    "            raise\n",
    "\n",
    "        return rmse\n",
    "\n",
    "\n",
    "    # Recherche des HP et prédictions\n",
    "    try:\n",
    "        study = optuna.create_study(direction=\"minimize\")\n",
    "        study.optimize(objective_sarimax, n_trials=num_trials, n_jobs=4)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(\n",
    "        \"Échec de la recherche d’HP. Paramètres : n_trials=%d, n_jobs=%d. Erreur : %s\",\n",
    "        num_trials, 4, str(e),\n",
    "        exc_info=True)\n",
    "        raise\n",
    "    \n",
    "    # Set optimal score/HP\n",
    "    dict_best_params = study.best_params\n",
    "    best_rmse = study.best_value\n",
    "\n",
    "    return best_rmse, dict_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ab03d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_cv_sarimax(outer_id: int, inner_folds: Dict, num_trials: int, inner_n_splits: int) -> Dict:\n",
    "    \n",
    "    \"\"\"Renvoie la liste optimale d'HP en effectuant une cross_validation interne \n",
    "    sur le fold (outer_id), avec un nombre d'essais (num_trials)\n",
    "\n",
    "    Args:\n",
    "        outer_id (int): ID du fold externe étudié\n",
    "        inner_folds (Dict): Folds internes\n",
    "        num_trials (int): Nombre d'essais Optuna\n",
    "        inner_n_split (int): Nombre de split dans tes outer splits\n",
    "\n",
    "    Returns:\n",
    "        fold_best_params (Dict) : Liste optimale d'HP pour le fold outer_id\n",
    "    \"\"\"\n",
    "\n",
    "    #Initialisation\n",
    "    inner_scores = []\n",
    "    inner_params = []\n",
    "    n_folds = inner_n_splits\n",
    "\n",
    "    # CV interne\n",
    "    for inner_id in range(n_folds):\n",
    "        X_train_inner, y_train_inner = inner_folds[f'train_{outer_id}_inner_train_{inner_id}']\n",
    "        X_val_inner, y_val_inner = inner_folds[f'train_{outer_id}_inner_val_{inner_id}']\n",
    "\n",
    "        best_rmse, best_params = train_sarimax(X_train=X_train_inner, \n",
    "                                                y_train=y_train_inner, \n",
    "                                                X_val=X_val_inner, \n",
    "                                                y_val=y_val_inner,\n",
    "                                                num_trials=num_trials)\n",
    "        # Scoring sur la CV interne\n",
    "        inner_scores.append(best_rmse)\n",
    "        inner_params.append(best_params)\n",
    "\n",
    "    # HP optimaux pour le outer fold concerné\n",
    "    best_inner_idx = np.argmin(inner_scores)\n",
    "    fold_best_params = inner_params[best_inner_idx]\n",
    "\n",
    "    return fold_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "974c5532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_cv_sarimax(X: pd.DataFrame,\n",
    "                      y: pd.Series,\n",
    "                      num_trials: int,\n",
    "                      outer_n_splits: int = 5,\n",
    "                      inner_n_splits: int = 3) -> tuple:\n",
    "    \n",
    "    \"\"\"Renvoie le RMSE moyen de l'ensemble des folds sur un modèle SARIMAX,\n",
    "    avec recherche d'hyperparamètres par nested CV.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): Variables explicatives\n",
    "        y (pd.Series): Variable cible\n",
    "        num_trials (int): Nombre d'essais Optuna pour l'optimisation\n",
    "        outer_n_splits (int): Nombre de splits externes\n",
    "        inner_n_splits (int): Nombre de splits internes\n",
    "\n",
    "    Returns:\n",
    "        tuple: (outer_scores, mean_outer_score)\n",
    "            - outer_scores : Liste des RMSE par fold externe\n",
    "            - mean_outer_score : Moyenne des RMSE externes\n",
    "    \"\"\"\n",
    "    # Création des folds externes et internes\n",
    "    outer_folds = outer_time_series_fold(X, y, outer_n_splits=outer_n_splits)\n",
    "    inner_folds = inner_time_series_fold(outer_folds, inner_n_splits=inner_n_splits)\n",
    "    outer_scores = []\n",
    "\n",
    "    for outer_id in range(outer_n_splits):\n",
    "        \n",
    "        # Séparation train/val externe\n",
    "        X_train_outer, y_train_outer = outer_folds[f\"train_{outer_id}\"]\n",
    "        X_val_outer, y_val_outer = outer_folds[f\"val_{outer_id}\"]\n",
    "\n",
    "        # Sélection des meilleurs hyperparamètres via inner CV\n",
    "        fold_best_params = inner_cv_sarimax(\n",
    "            outer_id=outer_id,\n",
    "            inner_folds=inner_folds,\n",
    "            num_trials=num_trials,\n",
    "            inner_n_splits=inner_n_splits\n",
    "        )\n",
    "\n",
    "        # Instanciation du meilleur modèle SARIMAX\n",
    "        best_model = SARIMAX(\n",
    "            endog=y_train_outer,\n",
    "            exog=X_train_outer,\n",
    "            order=fold_best_params[\"order\"],\n",
    "            seasonal_order=fold_best_params.get(\"seasonal_order\", (0, 0, 0, 0)),\n",
    "        )\n",
    "\n",
    "        # Fit du modèle\n",
    "        fitted_model = best_model.fit(disp=False)\n",
    "\n",
    "        # Prédiction sur la validation externe\n",
    "        y_pred_outer = fitted_model.get_prediction(\n",
    "            start=y_val_outer.index[0],\n",
    "            end=y_val_outer.index[-1],\n",
    "            exog=X_val_outer\n",
    "        )\n",
    "\n",
    "        # Calcul du RMSE externe\n",
    "        outer_rmse = np.sqrt(mean_squared_error(y_val_outer, y_pred_outer))\n",
    "        outer_scores.append(outer_rmse)\n",
    "\n",
    "    # Moyenne des scores externes\n",
    "    mean_outer_score = np.mean(outer_scores)\n",
    "    logging.info(f'RMSE global moyen attendu en production : {mean_outer_score}')\n",
    "\n",
    "    return outer_scores, mean_outer_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfbd89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 21:49:23,178] A new study created in memory with name: no-name-fd665e5c-95f3-4ae9-ab00-6f3d7a8996db\n",
      "d:\\Anaconda\\envs\\solar_forecasting\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "d:\\Anaconda\\envs\\solar_forecasting\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "d:\\Anaconda\\envs\\solar_forecasting\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "d:\\Anaconda\\envs\\solar_forecasting\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "d:\\Anaconda\\envs\\solar_forecasting\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "d:\\Anaconda\\envs\\solar_forecasting\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "d:\\Anaconda\\envs\\solar_forecasting\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "d:\\Anaconda\\envs\\solar_forecasting\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency h will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "[W 2025-08-18 21:52:05,663] Trial 2 failed with parameters: {'p': 3, 'q': 0, 'P': 0, 'D': 0} because of the following error: TypeError(\"Cannot convert input [                           temperature_2m  relative_humidity_2m  \\\\\\n2023-03-21 09:00:00+01:00       10.186000             74.919556   \\n2023-03-21 10:00:00+01:00       11.836000             60.710686   \\n2023-03-21 11:00:00+01:00       12.436000             54.982605   \\n2023-03-21 12:00:00+01:00       13.286000             52.376210   \\n2023-03-21 13:00:00+01:00       13.786000             52.879696   \\n...                                   ...                   ...   \\n2023-05-06 13:00:00+02:00       22.335999             55.824757   \\n2023-05-06 14:00:00+02:00       22.736000             53.426760   \\n2023-05-06 15:00:00+02:00       23.335999             52.029984   \\n2023-05-06 16:00:00+02:00       23.036000             54.917408   \\n2023-05-06 17:00:00+02:00       22.286000             58.988064   \\n\\n                           precipitation  surface_pressure  cloud_cover  \\\\\\n2023-03-21 09:00:00+01:00            0.0         964.19946        100.0   \\n2023-03-21 10:00:00+01:00            0.0         964.79315        100.0   \\n2023-03-21 11:00:00+01:00            0.0         965.09420        100.0   \\n2023-03-21 12:00:00+01:00            0.0         964.87330        100.0   \\n2023-03-21 13:00:00+01:00            0.0         964.68164        100.0   \\n...                                  ...               ...          ...   \\n2023-05-06 13:00:00+02:00            0.0         961.85187         49.0   \\n2023-05-06 14:00:00+02:00            0.0         961.44700         82.0   \\n2023-05-06 15:00:00+02:00            0.0         961.17120         81.0   \\n2023-05-06 16:00:00+02:00            0.0         960.74030         97.0   \\n2023-05-06 17:00:00+02:00            0.0         960.32670         96.0   \\n\\n                           wind_speed_10m  wind_direction_10m  \\\\\\n2023-03-21 09:00:00+01:00        4.334974           131.63345   \\n2023-03-21 10:00:00+01:00        4.024922           280.30478   \\n2023-03-21 11:00:00+01:00        6.608722           299.35767   \\n2023-03-21 12:00:00+01:00        6.489992           289.44012   \\n2023-03-21 13:00:00+01:00        7.421590           292.83368   \\n...                                   ...                 ...   \\n2023-05-06 13:00:00+02:00       13.746156           135.00010   \\n2023-05-06 14:00:00+02:00       14.512064           133.99501   \\n2023-05-06 15:00:00+02:00       15.038350           137.91092   \\n2023-05-06 16:00:00+02:00       16.087610           130.46214   \\n2023-05-06 17:00:00+02:00       15.188417           121.42952   \\n\\n                           global_tilted_irradiance  \\\\\\n2023-03-21 09:00:00+01:00                 196.99998   \\n2023-03-21 10:00:00+01:00                 330.00000   \\n2023-03-21 11:00:00+01:00                 466.00000   \\n2023-03-21 12:00:00+01:00                 583.00000   \\n2023-03-21 13:00:00+01:00                 632.00000   \\n...                                             ...   \\n2023-05-06 13:00:00+02:00                 839.00000   \\n2023-05-06 14:00:00+02:00                 899.00000   \\n2023-05-06 15:00:00+02:00                 890.00000   \\n2023-05-06 16:00:00+02:00                 814.00000   \\n2023-05-06 17:00:00+02:00                 634.00000   \\n\\n                           global_tilted_irradiance_delta_minmax  \\\\\\n2023-03-21 09:00:00+01:00                                   49.0   \\n2023-03-21 10:00:00+01:00                                   60.0   \\n2023-03-21 11:00:00+01:00                                   91.0   \\n2023-03-21 12:00:00+01:00                                  108.0   \\n2023-03-21 13:00:00+01:00                                  220.0   \\n...                                                          ...   \\n2023-05-06 13:00:00+02:00                                  144.0   \\n2023-05-06 14:00:00+02:00                                  157.0   \\n2023-05-06 15:00:00+02:00                                  283.0   \\n2023-05-06 16:00:00+02:00                                  278.0   \\n2023-05-06 17:00:00+02:00                                  216.0   \\n\\n                           global_tilted_irradiance_std  is_day  \\n2023-03-21 09:00:00+01:00                       17.8412     1.0  \\n2023-03-21 10:00:00+01:00                       17.6370     1.0  \\n2023-03-21 11:00:00+01:00                       25.9395     1.0  \\n2023-03-21 12:00:00+01:00                       30.5014     1.0  \\n2023-03-21 13:00:00+01:00                       57.7540     1.0  \\n...                                                 ...     ...  \\n2023-05-06 13:00:00+02:00                       36.5455     1.0  \\n2023-05-06 14:00:00+02:00                       43.7749     1.0  \\n2023-05-06 15:00:00+02:00                       73.8962     1.0  \\n2023-05-06 16:00:00+02:00                       91.2087     1.0  \\n2023-05-06 17:00:00+02:00                       75.2557     1.0  \\n\\n[1112 rows x 11 columns]] of type <class 'pandas.core.frame.DataFrame'> to Timestamp\").\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\solar_forecasting\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Simon\\AppData\\Local\\Temp\\ipykernel_23076\\3159826258.py\", line 56, in objective_sarimax\n",
      "    predictions = fitted_model.get_prediction(X_val)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\envs\\solar_forecasting\\Lib\\site-packages\\statsmodels\\tsa\\statespace\\mlemodel.py\", line 3341, in get_prediction\n",
      "    self.model._get_prediction_index(start, end, index))\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\envs\\solar_forecasting\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py\", line 837, in _get_prediction_index\n",
      "    return get_prediction_index(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\envs\\solar_forecasting\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py\", line 358, in get_prediction_index\n",
      "    start, _, start_oos = get_index_label_loc(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\envs\\solar_forecasting\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py\", line 245, in get_index_label_loc\n",
      "    loc, index, index_was_expanded = get_index_loc(key, index)\n",
      "                                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\envs\\solar_forecasting\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py\", line 154, in get_index_loc\n",
      "    date_key = Timestamp(key)\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"timestamps.pyx\", line 1865, in pandas._libs.tslibs.timestamps.Timestamp.__new__\n",
      "  File \"conversion.pyx\", line 425, in pandas._libs.tslibs.conversion.convert_to_tsobject\n",
      "TypeError: Cannot convert input [                           temperature_2m  relative_humidity_2m  \\\n",
      "2023-03-21 09:00:00+01:00       10.186000             74.919556   \n",
      "2023-03-21 10:00:00+01:00       11.836000             60.710686   \n",
      "2023-03-21 11:00:00+01:00       12.436000             54.982605   \n",
      "2023-03-21 12:00:00+01:00       13.286000             52.376210   \n",
      "2023-03-21 13:00:00+01:00       13.786000             52.879696   \n",
      "...                                   ...                   ...   \n",
      "2023-05-06 13:00:00+02:00       22.335999             55.824757   \n",
      "2023-05-06 14:00:00+02:00       22.736000             53.426760   \n",
      "2023-05-06 15:00:00+02:00       23.335999             52.029984   \n",
      "2023-05-06 16:00:00+02:00       23.036000             54.917408   \n",
      "2023-05-06 17:00:00+02:00       22.286000             58.988064   \n",
      "\n",
      "                           precipitation  surface_pressure  cloud_cover  \\\n",
      "2023-03-21 09:00:00+01:00            0.0         964.19946        100.0   \n",
      "2023-03-21 10:00:00+01:00            0.0         964.79315        100.0   \n",
      "2023-03-21 11:00:00+01:00            0.0         965.09420        100.0   \n",
      "2023-03-21 12:00:00+01:00            0.0         964.87330        100.0   \n",
      "2023-03-21 13:00:00+01:00            0.0         964.68164        100.0   \n",
      "...                                  ...               ...          ...   \n",
      "2023-05-06 13:00:00+02:00            0.0         961.85187         49.0   \n",
      "2023-05-06 14:00:00+02:00            0.0         961.44700         82.0   \n",
      "2023-05-06 15:00:00+02:00            0.0         961.17120         81.0   \n",
      "2023-05-06 16:00:00+02:00            0.0         960.74030         97.0   \n",
      "2023-05-06 17:00:00+02:00            0.0         960.32670         96.0   \n",
      "\n",
      "                           wind_speed_10m  wind_direction_10m  \\\n",
      "2023-03-21 09:00:00+01:00        4.334974           131.63345   \n",
      "2023-03-21 10:00:00+01:00        4.024922           280.30478   \n",
      "2023-03-21 11:00:00+01:00        6.608722           299.35767   \n",
      "2023-03-21 12:00:00+01:00        6.489992           289.44012   \n",
      "2023-03-21 13:00:00+01:00        7.421590           292.83368   \n",
      "...                                   ...                 ...   \n",
      "2023-05-06 13:00:00+02:00       13.746156           135.00010   \n",
      "2023-05-06 14:00:00+02:00       14.512064           133.99501   \n",
      "2023-05-06 15:00:00+02:00       15.038350           137.91092   \n",
      "2023-05-06 16:00:00+02:00       16.087610           130.46214   \n",
      "2023-05-06 17:00:00+02:00       15.188417           121.42952   \n",
      "\n",
      "                           global_tilted_irradiance  \\\n",
      "2023-03-21 09:00:00+01:00                 196.99998   \n",
      "2023-03-21 10:00:00+01:00                 330.00000   \n",
      "2023-03-21 11:00:00+01:00                 466.00000   \n",
      "2023-03-21 12:00:00+01:00                 583.00000   \n",
      "2023-03-21 13:00:00+01:00                 632.00000   \n",
      "...                                             ...   \n",
      "2023-05-06 13:00:00+02:00                 839.00000   \n",
      "2023-05-06 14:00:00+02:00                 899.00000   \n",
      "2023-05-06 15:00:00+02:00                 890.00000   \n",
      "2023-05-06 16:00:00+02:00                 814.00000   \n",
      "2023-05-06 17:00:00+02:00                 634.00000   \n",
      "\n",
      "                           global_tilted_irradiance_delta_minmax  \\\n",
      "2023-03-21 09:00:00+01:00                                   49.0   \n",
      "2023-03-21 10:00:00+01:00                                   60.0   \n",
      "2023-03-21 11:00:00+01:00                                   91.0   \n",
      "2023-03-21 12:00:00+01:00                                  108.0   \n",
      "2023-03-21 13:00:00+01:00                                  220.0   \n",
      "...                                                          ...   \n",
      "2023-05-06 13:00:00+02:00                                  144.0   \n",
      "2023-05-06 14:00:00+02:00                                  157.0   \n",
      "2023-05-06 15:00:00+02:00                                  283.0   \n",
      "2023-05-06 16:00:00+02:00                                  278.0   \n",
      "2023-05-06 17:00:00+02:00                                  216.0   \n",
      "\n",
      "                           global_tilted_irradiance_std  is_day  \n",
      "2023-03-21 09:00:00+01:00                       17.8412     1.0  \n",
      "2023-03-21 10:00:00+01:00                       17.6370     1.0  \n",
      "2023-03-21 11:00:00+01:00                       25.9395     1.0  \n",
      "2023-03-21 12:00:00+01:00                       30.5014     1.0  \n",
      "2023-03-21 13:00:00+01:00                       57.7540     1.0  \n",
      "...                                                 ...     ...  \n",
      "2023-05-06 13:00:00+02:00                       36.5455     1.0  \n",
      "2023-05-06 14:00:00+02:00                       43.7749     1.0  \n",
      "2023-05-06 15:00:00+02:00                       73.8962     1.0  \n",
      "2023-05-06 16:00:00+02:00                       91.2087     1.0  \n",
      "2023-05-06 17:00:00+02:00                       75.2557     1.0  \n",
      "\n",
      "[1112 rows x 11 columns]] of type <class 'pandas.core.frame.DataFrame'> to Timestamp\n",
      "[W 2025-08-18 21:52:07,254] Trial 2 failed with value None.\n"
     ]
    }
   ],
   "source": [
    "# Normalization\n",
    "X = df[feature_sarimax]\n",
    "X = pd.merge(X, index_df, left_index=True, right_index=True, how=\"left\").fillna(0)\n",
    "normalized_solar_ts = df[\"solar_mw\"]/df_power[\"chronique_capacity\"]\n",
    "normalized_solar_ts = normalized_solar_ts.dropna()\n",
    "y = normalized_solar_ts.copy()\n",
    "\n",
    "# Nested CV\n",
    "outer_n_splits = 5\n",
    "inner_n_splits = 2\n",
    "num_trials = 10\n",
    "outer_scores, mean_outer_score = nested_cv_sarimax(X=X, \n",
    "                                                    y=y, \n",
    "                                                    num_trials=num_trials, \n",
    "                                                    outer_n_splits=outer_n_splits, \n",
    "                                                    inner_n_splits=inner_n_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6be3f4",
   "metadata": {},
   "source": [
    "### Modèle LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d313d67a",
   "metadata": {},
   "source": [
    "Le modèle LightGBM est un modèle par arbre qui est particulièrement intéressant dans le cadre de la compréhension de données bruitées et des interactions non linéaires. Nous pourrions aussi le comparer à XGBoost, qui a la faculté d'éviter plus souvent l'overfitting (à plus long terme). Le RMSE sera pris comme métrique de minimisation pour permettre une meilleure prise en compte des évènements extrêmes. Le MAE, MAPE et SMAPE seront aussi retournés, cette fois à titre indicatif pour comparaison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de19e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lightgbm(X_train: pd.DataFrame, \n",
    "                   y_train: pd.Series, \n",
    "                   X_val: pd.DataFrame, \n",
    "                   y_val: pd.Series, \n",
    "                   num_trials: Optional[int|None]) -> tuple:\n",
    "    \n",
    "    \"\"\"Entrainement d'un modèle LightGBM avec nombre d'essais pour optimisation.\n",
    "    Retourne la quantification de son erreur (best_rmse) et ses hyperparamètres \n",
    "    optimaux (dict_best_params) sur un dataset de validation (X_val, y_val)\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Dataset d'entrainement\n",
    "        y_train (pd.Series): Variable cible d'entrainement\n",
    "        X_val (pd.DataFrame): Dataset de validation\n",
    "        y_val (pd.Series): Variable cible de validation\n",
    "\n",
    "    Returns:\n",
    "        best_rmse (float), dict_best_params (Dict) : Erreur (best_rmse) \n",
    "        et ses hyperparamètres optimaux (dict_best_params)\n",
    "    \"\"\"\n",
    "    # Initialisation\n",
    "    best_rmse = np.inf\n",
    "    dict_best_params = {}\n",
    "    \n",
    "    def objective_lightgbm(trial) -> np.float64 :\n",
    "        \"\"\"Prend en entrée un set d'hyperparamètres LightGBM issus du sampler d'Optuna, \n",
    "        et retourne le RMSE associé.\n",
    "\n",
    "        Args:\n",
    "            trial : Set d'hyperparamètres LightGBM\n",
    "\n",
    "        Returns:\n",
    "            rmse (np.float64): Racine carrée de l'erreur quadratique moyenne du modèle\n",
    "        \"\"\"\n",
    "        # HP\n",
    "        num_leaves = trial.suggest_int(\"num_leaves\", 10, 100)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 3, 15)\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 0.005, 0.2, log=True)\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 100, 1000)\n",
    "        min_child_samples = trial.suggest_int(\"min_child_samples\", 10, 50)\n",
    "\n",
    "        # Model\n",
    "        model = lgb.LGBMRegressor(num_leaves=num_leaves, \n",
    "                                max_depth=max_depth, \n",
    "                                learning_rate=learning_rate,\n",
    "                                n_estimators=n_estimators,\n",
    "                                min_child_samples=min_child_samples, verbosity=-1,\n",
    "                                random_state=42)\n",
    "        \n",
    "        # Training\n",
    "        try:\n",
    "            fitted_model = model.fit(X_train, y_train)\n",
    "            predictions = fitted_model.predict(X_val)\n",
    "            rmse = np.sqrt(mean_squared_error(y_val, predictions))\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.info(\"Echec de l'entrainement du modèle%s\", e)\n",
    "            logging.debug(\"Détails complets :\\n%s\", traceback.format_exc())\n",
    "            raise\n",
    "\n",
    "        return rmse\n",
    "\n",
    "\n",
    "    # Recherche des HP et prédictions\n",
    "    try:\n",
    "        study = optuna.create_study(direction=\"minimize\")\n",
    "        study.optimize(objective_lightgbm, n_trials=num_trials, n_jobs=4)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(\n",
    "        \"Échec de la recherche d’HP. Paramètres : n_trials=%d, n_jobs=%d. Erreur : %s\",\n",
    "        num_trials, 4, str(e),\n",
    "        exc_info=True)\n",
    "        raise\n",
    "    \n",
    "    # Set optimal score/HP\n",
    "    dict_best_params = study.best_params\n",
    "    best_rmse = study.best_value\n",
    "\n",
    "    return best_rmse, dict_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc9512e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_cv_lightgbm(outer_id: int, inner_folds: Dict, num_trials: int, inner_n_splits: int) -> Dict:\n",
    "    \n",
    "    \"\"\"Renvoie la liste optimale d'HP en effectuant une cross_validation interne \n",
    "    sur le fold (outer_id), avec un nombre d'essais (num_trials)\n",
    "\n",
    "    Args:\n",
    "        outer_id (int): ID du fold externe étudié\n",
    "        inner_folds (Dict): Folds internes\n",
    "        num_trials (int): Nombre d'essais Optuna\n",
    "        inner_n_split (int): Nombre de split dans tes outer splits\n",
    "\n",
    "    Returns:\n",
    "        fold_best_params (Dict) : Liste optimale d'HP pour le fold outer_id\n",
    "    \"\"\"\n",
    "\n",
    "    #Initialisation\n",
    "    inner_scores = []\n",
    "    inner_params = []\n",
    "    n_folds = inner_n_splits\n",
    "\n",
    "    # CV interne\n",
    "    for inner_id in range(n_folds):\n",
    "        X_train_inner, y_train_inner = inner_folds[f'train_{outer_id}_inner_train_{inner_id}']\n",
    "        X_val_inner, y_val_inner = inner_folds[f'train_{outer_id}_inner_val_{inner_id}']\n",
    "\n",
    "        best_rmse, best_params = train_lightgbm(X_train=X_train_inner, \n",
    "                                                y_train=y_train_inner, \n",
    "                                                X_val=X_val_inner, \n",
    "                                                y_val=y_val_inner,\n",
    "                                                num_trials=num_trials)\n",
    "        # Scoring sur la CV interne\n",
    "        inner_scores.append(best_rmse)\n",
    "        inner_params.append(best_params)\n",
    "\n",
    "    # HP optimaux pour le outer fold concerné\n",
    "    best_inner_idx = np.argmin(inner_scores)\n",
    "    fold_best_params = inner_params[best_inner_idx]\n",
    "\n",
    "    return fold_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b714221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_cv_lightgbm(X : pd.DataFrame, y: pd.Series, num_trials: int, outer_n_splits: int = 5, inner_n_splits: int = 3) -> tuple:\n",
    "    \"\"\"Renvoie le RMSE moyen de l'ensemble des folds sur un modèle LightGBM, avec nombre d'essais d'HP.\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): Dataset d'entrainement\n",
    "        y (pd.DataFrame): Variable cible\n",
    "        num_trials (int): Nombre d'essais Optuna\n",
    "        outer_n_splits (int): Nombre de split dans ton dataset X\n",
    "        inner_n_split (int): Nombre de split dans tes outer splits\n",
    "\n",
    "    Returns:\n",
    "        outer_scores, mean_outer_score (tuple) : Liste de l'ensemble des scores \n",
    "        des différents folds externe - Moyenne de l'ensemble\n",
    "    \"\"\"\n",
    "    # Initialisation\n",
    "    outer_folds = outer_time_series_fold(X, y, outer_n_splits=outer_n_splits)\n",
    "    inner_folds = inner_time_series_fold(outer_folds, inner_n_splits=inner_n_splits)\n",
    "    outer_scores = []\n",
    "\n",
    "    for outer_id in range(outer_n_splits):\n",
    "        \n",
    "        # Inner CV\n",
    "        X_train_outer, y_train_outer = outer_folds[f\"train_{outer_id}\"]\n",
    "        X_val_outer, y_val_outer = outer_folds[f\"val_{outer_id}\"]\n",
    "        fold_best_params = inner_cv_lightgbm(outer_id=outer_id, \n",
    "                                             inner_folds=inner_folds,\n",
    "                                               num_trials=num_trials, \n",
    "                                               inner_n_splits=inner_n_splits)\n",
    "\n",
    "        # Best outer model\n",
    "        best_model = lgb.LGBMRegressor(**fold_best_params, verbosity=-1, random_state=42)\n",
    "        fitted_model = best_model.fit(X=X_train_outer, y=y_train_outer)\n",
    "        \n",
    "        # Scoring\n",
    "        y_pred_outer = fitted_model.predict(X_val_outer)\n",
    "        outer_rmse = np.sqrt(mean_squared_error(y_val_outer, y_pred_outer))\n",
    "        outer_scores.append(outer_rmse)\n",
    "    \n",
    "    # Scoring final\n",
    "    mean_outer_score = np.mean(outer_scores)\n",
    "    logging.info(f'RMSE global moyen attendu en production : {mean_outer_score}')\n",
    "    \n",
    "    return outer_scores, mean_outer_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03545d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 19:54:49,706] A new study created in memory with name: no-name-5b10c5c0-b31f-41a8-82ba-168fbe5b2118\n",
      "[I 2025-08-18 19:54:55,560] Trial 2 finished with value: 0.07203506342219206 and parameters: {'num_leaves': 45, 'max_depth': 10, 'learning_rate': 0.04196287072294732, 'n_estimators': 193, 'min_child_samples': 27}. Best is trial 2 with value: 0.07203506342219206.\n",
      "[I 2025-08-18 19:54:55,708] Trial 3 finished with value: 0.0851073013503694 and parameters: {'num_leaves': 49, 'max_depth': 7, 'learning_rate': 0.008127588957913686, 'n_estimators': 252, 'min_child_samples': 31}. Best is trial 2 with value: 0.07203506342219206.\n",
      "[I 2025-08-18 19:54:56,483] Trial 10 finished with value: 0.07494824325406045 and parameters: {'num_leaves': 25, 'max_depth': 13, 'learning_rate': 0.0767098680559528, 'n_estimators': 272, 'min_child_samples': 46}. Best is trial 2 with value: 0.07203506342219206.\n",
      "[I 2025-08-18 19:54:57,046] Trial 9 finished with value: 0.07256084348583083 and parameters: {'num_leaves': 44, 'max_depth': 5, 'learning_rate': 0.016884695056685113, 'n_estimators': 366, 'min_child_samples': 20}. Best is trial 2 with value: 0.07203506342219206.\n",
      "[I 2025-08-18 19:54:57,746] Trial 6 finished with value: 0.07158576411551083 and parameters: {'num_leaves': 86, 'max_depth': 14, 'learning_rate': 0.06621801453285192, 'n_estimators': 253, 'min_child_samples': 29}. Best is trial 6 with value: 0.07158576411551083.\n",
      "[I 2025-08-18 19:54:57,797] Trial 14 finished with value: 0.07425594026845096 and parameters: {'num_leaves': 10, 'max_depth': 4, 'learning_rate': 0.029224002899087928, 'n_estimators': 189, 'min_child_samples': 38}. Best is trial 6 with value: 0.07158576411551083.\n",
      "[I 2025-08-18 19:54:58,953] Trial 7 finished with value: 0.07326145015431869 and parameters: {'num_leaves': 99, 'max_depth': 9, 'learning_rate': 0.06775713367563334, 'n_estimators': 591, 'min_child_samples': 46}. Best is trial 6 with value: 0.07158576411551083.\n",
      "[I 2025-08-18 19:54:59,206] Trial 15 finished with value: 0.07656982610947358 and parameters: {'num_leaves': 55, 'max_depth': 3, 'learning_rate': 0.07857614244278092, 'n_estimators': 468, 'min_child_samples': 11}. Best is trial 6 with value: 0.07158576411551083.\n",
      "[I 2025-08-18 19:54:59,317] Trial 13 finished with value: 0.07359650137383227 and parameters: {'num_leaves': 85, 'max_depth': 7, 'learning_rate': 0.19116047069902925, 'n_estimators': 323, 'min_child_samples': 30}. Best is trial 6 with value: 0.07158576411551083.\n",
      "[I 2025-08-18 19:54:59,344] Trial 16 finished with value: 0.09790963356461767 and parameters: {'num_leaves': 89, 'max_depth': 5, 'learning_rate': 0.013582818442765944, 'n_estimators': 104, 'min_child_samples': 15}. Best is trial 6 with value: 0.07158576411551083.\n",
      "[I 2025-08-18 19:54:59,637] Trial 5 finished with value: 0.08167864381108324 and parameters: {'num_leaves': 35, 'max_depth': 12, 'learning_rate': 0.0052519429314104825, 'n_estimators': 471, 'min_child_samples': 44}. Best is trial 6 with value: 0.07158576411551083.\n",
      "[I 2025-08-18 19:55:01,381] Trial 0 finished with value: 0.07202925654129624 and parameters: {'num_leaves': 98, 'max_depth': 13, 'learning_rate': 0.023959760455982713, 'n_estimators': 451, 'min_child_samples': 31}. Best is trial 6 with value: 0.07158576411551083.\n",
      "[I 2025-08-18 19:55:01,783] Trial 12 finished with value: 0.07711659742188498 and parameters: {'num_leaves': 67, 'max_depth': 6, 'learning_rate': 0.021098869613399983, 'n_estimators': 272, 'min_child_samples': 10}. Best is trial 6 with value: 0.07158576411551083.\n",
      "[I 2025-08-18 19:55:03,071] Trial 8 finished with value: 0.07246782432232336 and parameters: {'num_leaves': 18, 'max_depth': 7, 'learning_rate': 0.006994175733617964, 'n_estimators': 855, 'min_child_samples': 27}. Best is trial 6 with value: 0.07158576411551083.\n",
      "[I 2025-08-18 19:55:03,092] Trial 17 finished with value: 0.07164650325638527 and parameters: {'num_leaves': 35, 'max_depth': 10, 'learning_rate': 0.08585274004654299, 'n_estimators': 286, 'min_child_samples': 30}. Best is trial 6 with value: 0.07158576411551083.\n",
      "[I 2025-08-18 19:55:06,338] Trial 11 finished with value: 0.07503072081886152 and parameters: {'num_leaves': 64, 'max_depth': 8, 'learning_rate': 0.1455129145694915, 'n_estimators': 952, 'min_child_samples': 31}. Best is trial 6 with value: 0.07158576411551083.\n",
      "[I 2025-08-18 19:55:06,955] Trial 19 finished with value: 0.07229513633247378 and parameters: {'num_leaves': 15, 'max_depth': 9, 'learning_rate': 0.08262466560044389, 'n_estimators': 485, 'min_child_samples': 19}. Best is trial 6 with value: 0.07158576411551083.\n",
      "[I 2025-08-18 19:55:09,981] Trial 20 finished with value: 0.0706776079228908 and parameters: {'num_leaves': 53, 'max_depth': 15, 'learning_rate': 0.15832223110282784, 'n_estimators': 483, 'min_child_samples': 28}. Best is trial 20 with value: 0.0706776079228908.\n",
      "[I 2025-08-18 19:55:10,244] Trial 18 finished with value: 0.07172171142950128 and parameters: {'num_leaves': 73, 'max_depth': 14, 'learning_rate': 0.06212277261270908, 'n_estimators': 343, 'min_child_samples': 13}. Best is trial 20 with value: 0.0706776079228908.\n",
      "[I 2025-08-18 19:55:11,943] Trial 4 finished with value: 0.07384168641439852 and parameters: {'num_leaves': 40, 'max_depth': 15, 'learning_rate': 0.13069274491973573, 'n_estimators': 842, 'min_child_samples': 26}. Best is trial 20 with value: 0.0706776079228908.\n",
      "[I 2025-08-18 19:55:13,953] Trial 1 finished with value: 0.07346466149887289 and parameters: {'num_leaves': 37, 'max_depth': 9, 'learning_rate': 0.009242354916258565, 'n_estimators': 812, 'min_child_samples': 14}. Best is trial 20 with value: 0.0706776079228908.\n",
      "[I 2025-08-18 19:55:15,767] Trial 25 finished with value: 0.07150294404374366 and parameters: {'num_leaves': 70, 'max_depth': 15, 'learning_rate': 0.15993625581065277, 'n_estimators': 672, 'min_child_samples': 37}. Best is trial 20 with value: 0.0706776079228908.\n",
      "[I 2025-08-18 19:55:15,939] Trial 26 finished with value: 0.07167504701036927 and parameters: {'num_leaves': 68, 'max_depth': 15, 'learning_rate': 0.18362388563589585, 'n_estimators': 675, 'min_child_samples': 38}. Best is trial 20 with value: 0.0706776079228908.\n",
      "[I 2025-08-18 19:55:16,905] Trial 28 finished with value: 0.06967614237593357 and parameters: {'num_leaves': 73, 'max_depth': 15, 'learning_rate': 0.04533919926068018, 'n_estimators': 626, 'min_child_samples': 39}. Best is trial 28 with value: 0.06967614237593357.\n",
      "[I 2025-08-18 19:55:17,130] Trial 23 finished with value: 0.07167079411363492 and parameters: {'num_leaves': 73, 'max_depth': 15, 'learning_rate': 0.03195965258551909, 'n_estimators': 806, 'min_child_samples': 24}. Best is trial 28 with value: 0.06967614237593357.\n",
      "[I 2025-08-18 19:55:17,335] Trial 24 finished with value: 0.0688424723607851 and parameters: {'num_leaves': 76, 'max_depth': 15, 'learning_rate': 0.14381919402781418, 'n_estimators': 842, 'min_child_samples': 36}. Best is trial 24 with value: 0.0688424723607851.\n",
      "[I 2025-08-18 19:55:17,601] Trial 21 finished with value: 0.07196834594796862 and parameters: {'num_leaves': 74, 'max_depth': 15, 'learning_rate': 0.1570771546729241, 'n_estimators': 849, 'min_child_samples': 22}. Best is trial 24 with value: 0.0688424723607851.\n",
      "[I 2025-08-18 19:55:17,731] Trial 22 finished with value: 0.07192860728741822 and parameters: {'num_leaves': 69, 'max_depth': 15, 'learning_rate': 0.03929333494324598, 'n_estimators': 862, 'min_child_samples': 25}. Best is trial 24 with value: 0.0688424723607851.\n",
      "[I 2025-08-18 19:55:17,845] Trial 27 finished with value: 0.07128715022376422 and parameters: {'num_leaves': 72, 'max_depth': 15, 'learning_rate': 0.04731735487216959, 'n_estimators': 590, 'min_child_samples': 21}. Best is trial 24 with value: 0.0688424723607851.\n",
      "[I 2025-08-18 19:55:18,024] Trial 29 finished with value: 0.06865284811830982 and parameters: {'num_leaves': 74, 'max_depth': 15, 'learning_rate': 0.13517756281908475, 'n_estimators': 693, 'min_child_samples': 38}. Best is trial 29 with value: 0.06865284811830982.\n",
      "[I 2025-08-18 19:55:18,029] A new study created in memory with name: no-name-cc236c8d-43e5-4015-ae6f-74f62d606b9a\n",
      "[I 2025-08-18 19:55:21,340] Trial 6 finished with value: 0.0603645019264532 and parameters: {'num_leaves': 60, 'max_depth': 3, 'learning_rate': 0.021535197995570552, 'n_estimators': 447, 'min_child_samples': 50}. Best is trial 6 with value: 0.0603645019264532.\n",
      "[I 2025-08-18 19:55:24,661] Trial 3 finished with value: 0.05973126361575564 and parameters: {'num_leaves': 10, 'max_depth': 9, 'learning_rate': 0.05872328184991598, 'n_estimators': 581, 'min_child_samples': 42}. Best is trial 3 with value: 0.05973126361575564.\n",
      "[I 2025-08-18 19:55:25,655] Trial 7 finished with value: 0.05516346722331918 and parameters: {'num_leaves': 67, 'max_depth': 4, 'learning_rate': 0.0066985195742841844, 'n_estimators': 652, 'min_child_samples': 41}. Best is trial 7 with value: 0.05516346722331918.\n",
      "[I 2025-08-18 19:55:26,957] Trial 8 finished with value: 0.0583108850845714 and parameters: {'num_leaves': 17, 'max_depth': 5, 'learning_rate': 0.08979310029920463, 'n_estimators': 736, 'min_child_samples': 34}. Best is trial 7 with value: 0.05516346722331918.\n",
      "[I 2025-08-18 19:55:27,693] Trial 9 finished with value: 0.05911675590971829 and parameters: {'num_leaves': 61, 'max_depth': 4, 'learning_rate': 0.026691857059173638, 'n_estimators': 890, 'min_child_samples': 42}. Best is trial 7 with value: 0.05516346722331918.\n",
      "[I 2025-08-18 19:55:28,955] Trial 4 finished with value: 0.057838236361312065 and parameters: {'num_leaves': 64, 'max_depth': 15, 'learning_rate': 0.061322327323438744, 'n_estimators': 316, 'min_child_samples': 48}. Best is trial 7 with value: 0.05516346722331918.\n",
      "[I 2025-08-18 19:55:31,421] Trial 15 finished with value: 0.05753300511683519 and parameters: {'num_leaves': 53, 'max_depth': 5, 'learning_rate': 0.029537525152004557, 'n_estimators': 215, 'min_child_samples': 31}. Best is trial 7 with value: 0.05516346722331918.\n",
      "[I 2025-08-18 19:55:31,452] Trial 13 finished with value: 0.057350336490804434 and parameters: {'num_leaves': 31, 'max_depth': 4, 'learning_rate': 0.03091588377771178, 'n_estimators': 552, 'min_child_samples': 12}. Best is trial 7 with value: 0.05516346722331918.\n",
      "[I 2025-08-18 19:55:33,550] Trial 5 finished with value: 0.058164072831717935 and parameters: {'num_leaves': 36, 'max_depth': 7, 'learning_rate': 0.015245159555157747, 'n_estimators': 739, 'min_child_samples': 38}. Best is trial 7 with value: 0.05516346722331918.\n",
      "[I 2025-08-18 19:55:37,094] Trial 17 finished with value: 0.05794670466325742 and parameters: {'num_leaves': 10, 'max_depth': 10, 'learning_rate': 0.1487585922815009, 'n_estimators': 523, 'min_child_samples': 42}. Best is trial 7 with value: 0.05516346722331918.\n",
      "[I 2025-08-18 19:55:37,826] Trial 2 finished with value: 0.057798177324234375 and parameters: {'num_leaves': 85, 'max_depth': 9, 'learning_rate': 0.16599416011323365, 'n_estimators': 726, 'min_child_samples': 50}. Best is trial 7 with value: 0.05516346722331918.\n",
      "[I 2025-08-18 19:55:38,004] Trial 11 finished with value: 0.057086545525517264 and parameters: {'num_leaves': 67, 'max_depth': 15, 'learning_rate': 0.012860139385390698, 'n_estimators': 453, 'min_child_samples': 38}. Best is trial 7 with value: 0.05516346722331918.\n",
      "[I 2025-08-18 19:55:38,142] Trial 0 finished with value: 0.05688351013786812 and parameters: {'num_leaves': 19, 'max_depth': 11, 'learning_rate': 0.09720656475235498, 'n_estimators': 859, 'min_child_samples': 37}. Best is trial 7 with value: 0.05516346722331918.\n",
      "[I 2025-08-18 19:55:38,853] Trial 12 finished with value: 0.0595154402788504 and parameters: {'num_leaves': 86, 'max_depth': 6, 'learning_rate': 0.012158926680806119, 'n_estimators': 858, 'min_child_samples': 42}. Best is trial 7 with value: 0.05516346722331918.\n",
      "[I 2025-08-18 19:55:44,831] Trial 10 finished with value: 0.05661785284888692 and parameters: {'num_leaves': 99, 'max_depth': 10, 'learning_rate': 0.02104915670448175, 'n_estimators': 758, 'min_child_samples': 26}. Best is trial 7 with value: 0.05516346722331918.\n",
      "[I 2025-08-18 19:55:45,077] Trial 16 finished with value: 0.055122504599244025 and parameters: {'num_leaves': 54, 'max_depth': 9, 'learning_rate': 0.03918433383844428, 'n_estimators': 387, 'min_child_samples': 18}. Best is trial 16 with value: 0.055122504599244025.\n",
      "[I 2025-08-18 19:55:45,440] Trial 1 finished with value: 0.05733298443032353 and parameters: {'num_leaves': 50, 'max_depth': 14, 'learning_rate': 0.061524937155494216, 'n_estimators': 941, 'min_child_samples': 49}. Best is trial 16 with value: 0.055122504599244025.\n",
      "[I 2025-08-18 19:55:46,748] Trial 20 finished with value: 0.056251376225163165 and parameters: {'num_leaves': 62, 'max_depth': 13, 'learning_rate': 0.1581469439877798, 'n_estimators': 368, 'min_child_samples': 25}. Best is trial 16 with value: 0.055122504599244025.\n"
     ]
    }
   ],
   "source": [
    "# Normalization\n",
    "normalized_solar_ts = df[\"solar_mw\"]/df_power[\"chronique_capacity\"]\n",
    "normalized_solar_ts = normalized_solar_ts.dropna()\n",
    "X = df.drop(columns=\"solar_mw\")\n",
    "y = normalized_solar_ts\n",
    "\n",
    "# Nested CV\n",
    "outer_n_splits = 5\n",
    "inner_n_splits = 2\n",
    "num_trials = 30\n",
    "outer_scores, mean_outer_score = nested_cv_lightgbm(X=X, \n",
    "                                                    y=y, \n",
    "                                                    num_trials=num_trials, \n",
    "                                                    outer_n_splits=outer_n_splits, \n",
    "                                                    inner_n_splits=inner_n_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a170efa3",
   "metadata": {},
   "source": [
    "### Modèle LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96db4959",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3871c3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En construction \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size: int,\n",
    "                 hidden_size: int,\n",
    "                 seq_length: int, \n",
    "                 num_layers: int, \n",
    "                 dropout: int\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_length = seq_length\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.projection = nn.Linear(input_size, hidden_size)\n",
    "        self.encoder = nn.LSTM(input_size=self.hidden_size,\n",
    "                               hidden_size=self.hidden_size,\n",
    "                               num_layers=self.num_layers,\n",
    "                               batch_first=True, \n",
    "                               dropout=self.dropout)\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_length, input_size)\n",
    "        hidden: optional tuple (h0, c0) of shape (num_layers, batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        device = x.device\n",
    "\n",
    "        # Projection\n",
    "        x = self.projection(x)  # (B, T, hidden_size)\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device)\n",
    "        \n",
    "        # Sortie\n",
    "        out, (hn, cn) = self.encoder(x, (h0, c0))\n",
    "        \n",
    "        return out, (hn, cn)\n",
    "        \n",
    "#%%\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size: int,\n",
    "                hidden_size: int,\n",
    "                out_seq_length: int, \n",
    "                batch_size: int,\n",
    "                num_layers: int,\n",
    "                dropout: int,\n",
    "                output_size: int\n",
    "                ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.out_seq_length = out_seq_length\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=self.input_size,\n",
    "                            hidden_size=self.hidden_size,\n",
    "                            num_layers=self.num_layers,\n",
    "                            batch_first=True,\n",
    "                            dropout=self.dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(in_features=self.hidden_size,\n",
    "                            out_features=self.output_size)\n",
    "\n",
    "    def forward(self, x, cn, hn):\n",
    "        out, (hn, cn) = self.lstm(x, (hn, cn))\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "solar_forecasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
